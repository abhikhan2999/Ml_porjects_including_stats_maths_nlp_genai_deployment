{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/tensorflow/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/tensorflow/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/tensorflow/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/tensorflow/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/tensorflow/\n",
      "ERROR: Could not find a version that satisfies the requirement TensorFlow (from versions: none)\n",
      "ERROR: No matching distribution found for TensorFlow\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m \n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "!pip install TensorFlow\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Deep Learning Methods\n",
    "Neural networks, especially autoencoders, can be effective in imputing missing values in complex datasets. Deep learning methods, particularly neural networks like autoencoders, offer a powerful approach for imputing missing values in complex datasets. These methods are especially useful when the data has intricate, non-linear relationships that traditional statistical methods might not capture effectively.\n",
    "\n",
    "Understanding Autoencoders for Imputation:\n",
    "What is an Autoencoder?\n",
    "\n",
    "An autoencoder is a type of neural network that is trained to copy its input to its output.\n",
    "It has a hidden layer that describes a code used to represent the input.\n",
    "The network may be viewed as consisting of two parts: an encoder function, which compresses the input into a latent-space representation, and a decoder function, which reconstructs the input from the latent space.\n",
    "How Autoencoders Work for Imputation:\n",
    "\n",
    "The key idea is to train the autoencoder to ignore the noise (missing values) in the input data.\n",
    "During training, inputs with missing values are presented, and the network learns to predict the missing values in a way that minimizes reconstruction error for known parts of the data.\n",
    "This results in the network learning a robust representation of the data, enabling it to make reasonable guesses about missing values.\n",
    "Advantages of Using Autoencoders:\n",
    "\n",
    "Handling Complex Patterns: They can capture non-linear relationships in the data, which is particularly useful for complex datasets.\n",
    "Scalability: They can handle large-scale datasets efficiently.\n",
    "Flexibility: They can be adapted to different types of data (e.g., images, text, time-series).\n",
    "Implementation Considerations:\n",
    "\n",
    "Data Preprocessing: Data should be normalized or standardized before feeding it into an autoencoder.\n",
    "Network Architecture: The choice of architecture (number of layers, type of layers, etc.) depends on the complexity of the data.\n",
    "Training Process: It might involve techniques like dropout or noise addition to improve the model's ability to handle missing data.\n",
    "Example Use-Cases:\n",
    "\n",
    "Image Data: Filling in missing pixels or reconstructing corrupted images.\n",
    "Time-Series Data: Imputing missing values in sequences like stock prices or weather data.\n",
    "Tabular Data: Handling missing entries in datasets used for machine learning.\n",
    "Implementation Example:\n",
    "Here's a simplified example of how you might set up an autoencoder for imputation in Python using TensorFlow and Keras: (Check the next notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#handling missing values and categorical values \u001b[39;00m\n\u001b[0;32m     15\u001b[0m numeric_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfare\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msibsp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 17\u001b[0m numeric_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m(steps \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     18\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m , MinMaxScaler\n\u001b[0;32m     20\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "#import titanic dataset \n",
    "\n",
    "df_titanic = sns.load_dataset('titanic')\n",
    "\n",
    "df_titanic = df_titanic[['survived', 'pclass','sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
    "\n",
    "#preprocessing \n",
    "#sperate features and target \n",
    "\n",
    "X = df_titanic.drop('survived', axis = 1)\n",
    "y = df_titanic['survived']\n",
    "\n",
    "#handling missing values and categorical values \n",
    "\n",
    "numeric_features = ['age', 'fare', 'sibsp', 'parch']\n",
    "\n",
    "numeric_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'mean')),\n",
    "    'scaler' , MinMaxScaler\n",
    "])\n",
    "\n",
    "\n",
    "#columntransformer for preprocessing \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformer = [\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)]\n",
    ")\n",
    "\n",
    "#perprocessing the dataset \n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "#splitting the dataset (we'll use the train test set to train the autoencoder)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size =0.2, random_state = 42)\n",
    "\n",
    "#define the autoecoder architecture \n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation = 'relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation = 'sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "#train the autoencoder \n",
    "autoencoder.fit(X_train, X_test, epochs = 50, batch_size = 256, shuffle = True, validation_split = 0.2)\n",
    "#using the autoencoder for the \n",
    "X_test_imputed = autoencoder.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
